set.seed(12345)
# LASSO
# ----------
sl_lasso <- SuperLearner(Y = y_train,              # target
X = x_train,              # features
family = binomial(),      # binomial : 1,0s
SL.library = "SL.glmnet") # find the glmnet algo from SL --> lasso algorithm
# view
sl_lasso
# Here is the risk of the best model (discrete SuperLearner winner).
# Use which.min boolean to find minimum cvRisk in list
sl_lasso$cvRisk[which.min(sl_lasso$cvRisk)]
# set seed
set.seed(987)
# multiple models
# ----------
sl = SuperLearner(Y = y_train,
X = x_train,
family = binomial(),
# notice these models are concatenated (vector of model names, after c())
SL.library = c('SL.mean',    #if you just guessed the average - serves as a baseline. good to include every time
'SL.glmnet',  #lasso
'SL.ranger')) #random forest
sl
# predictions
# ----------
preds <-
predict(sl,             # use the superlearner not individual models
x_test,         # prediction on test set
onlySL = TRUE)  # use only models that were found to be useful (had weights)
# start with y_test
validation <-
y_test %>%
# add our predictions - first column of predictions
bind_cols(preds$pred[,1]) %>%
# rename columns
rename(obs = `...1`,      # actual observations --> r names them ...1 and ...2, we're renaming them something useful
pred = `...2`) %>% # predicted prob
# change pred column so that obs above .5 are 1, otherwise 0
mutate(pred = ifelse(pred >= .5,
1,
0))
# view
head(validation)
# confusion matrix
# ----------
caret::confusionMatrix(as.factor(validation$pred),
as.factor(validation$obs))
boston_split
print(64+7+10+46)
#
# Macs/Linux
# --------------------------------------------------------
# identify number of cores to use
n_cores <- availableCores() - 1 # maybe 2 cores instead?
# plan separate session
plan(multisession,
workers = n_cores)
# set seed - option will set same seed across multiple cores
set.seed(44, "L'Ecuyer-CMRG")
# cross-validation across cores
# ----------
cv_sl = CV.SuperLearner(Y = y_train,
X = x_train,
family = binomial(),      #
V = 20,                   # default fold recommendation for Superlearner
parallel = 'multicore',   # macs linux parallel specification, note its a string, use one less core than you have on your computer
SL.library = c("SL.mean",
"SL.glmnet",
"SL.ranger"))
# plot
# plot(cv_sl)
# summary(cv_sl)
# glimpse(cv_sl)
# specify which SuperLearner algorithms we want to use
# ----------
sl_libs <- c('SL.glmnet', 'SL.ranger', 'SL.glm')
# Prepare data for SuperLearner/TMLE
# ----------
# Mutate Y, A for outcome and treatment, use tax, age, and crim as covariates
data_obs <-
Boston %>%
mutate(Y = ifelse(medv > 22,
1,
0)) %>%
rename(A = chas) %>%
select(Y, A, tax, age, crim)  # A = borders the charles river
# Outcome
# ----------
Y <-
data_obs %>%
pull(Y)
# Covariates
# ----------
W_A <-
data_obs %>%
select(-Y)
# Fit SL for Q step, initial estimate of the outcome
# ----------
Q <- SuperLearner(Y = Y,                # outcome
X = W_A,              # covariates + treatment
family = binomial(),  # binominal bc outcome is binary
SL.library = sl_libs) # ML algorithms -- vector we created in first line of this chunk
# observed treatment
# ----------
Q_A <- as.vector(predict(Q)$pred) #actual treatment status
# if every unit was treated (pretending every unit was treated) creating treatment counterfactual using learned relationships from fitting above in the "Fit SL for Q step, initial estimate of the outcome" step
# ----------
W_A1 <- W_A %>% mutate(A = 1)
Q_1 <- as.vector(predict(Q, newdata = W_A1)$pred)
# if everyone was control (pretending every unit was not treated) creating control counterfactual
# ----------
W_A0 <- W_A %>% mutate(A = 0)
Q_0 <- as.vector(predict(Q, newdata = W_A0)$pred)
# combine all predictions into one dataframe
# ----------
dat_tmle <- tibble(Y = Y,
A = W_A$A,
Q_A,
Q_0,
Q_1)
# view
head(dat_tmle)
# G-computation
# ----------
ate_gcomp <- mean(dat_tmle$Q_1 - dat_tmle$Q_0)
ate_gcomp
# prepare data for analysis
# ----------
A <- W_A$A
W <- Boston %>% select(tax, age, crim)  # select jsut a few covariates
# model probability of treatment (similar to matching)
# ----------
g <- SuperLearner(Y = A,              # outcome is treatment
X = W,              # vector of covariates (predictors)
family=binomial(),  # binary outcome
SL.library=sl_libs) # models
# Prediction for probability of treatment
# ----------
g_w <- as.vector(predict(g)$pred) # Pr(A=1|W)
# probability of treatment: iptw : (inverse probability weight of receiving treatment)
# ----------
H_1 <- 1/g_w
# probability of control: niptw : (negative inverse probability weight of not receiving treatment)
# ----------
H_0 <- -1/(1-g_w)
# create a clever covariate
# ----------
dat_tmle <- # add clever covariate data to dat_tmle
dat_tmle %>%
bind_cols(
H_1 = H_1,
H_0 = H_0) %>%
# create clever covariate whereby case takes iptw or niptw based on treatment status
mutate(H_A = case_when(A == 1 ~ H_1,
A == 0 ~ H_0))
# fluctuation parameter
# ----------
glm_fit <- glm(Y ~ -1 + offset(qlogis(Q_A)) + H_A,
data=dat_tmle,
family=binomial)
glm_fit
# information we need to summarize probability of treatment step above -- how much to update
# ----------
eps <- coef(glm_fit)
eps
# add H_A variable
# ----------
H_A <- dat_tmle$H_A
# predictions for treatment status
# ----------
Q_A_update <- plogis(qlogis(Q_A) + eps*H_A) # updating by adding eps * H_A
# predictions assuming everyone under treatment
# ----------
Q_1_update <- plogis(qlogis(Q_1) + eps*H_1) # updating by adding eps * H_1
# predictions assuming everyone under  control
# ----------
Q_0_update <- plogis(qlogis(Q_0) + eps*H_0) # updating by adding eps * H_0
# calculate ATE
# ----------
tmle_ate <- mean(Q_1_update - Q_0_update)
tmle_ate
# calculate standard errors and p-values
# ----------
infl_fn <- (Y - Q_A_update) * H_A + Q_1_update - Q_0_update - tmle_ate
# calculate ATE
# ----------
tmle_se <- sqrt(var(infl_fn)/nrow(data_obs))
# confidence intervals
conf_low <- tmle_ate - 1.96*tmle_se
conf_high <- tmle_ate + 1.96*tmle_se
# p-values
pval <- 2 * (1 - pnorm(abs(tmle_ate / tmle_se)))
# view
tmle_ate
conf_low
conf_high
pval
# set seed for reproducibility
set.seed(1000)
# implement above all in one step using tmle
# ----------
tmle_fit <-
tmle::tmle(Y = Y,                  # outcome
A = A,                  # treatment
W = W,                  # baseline covariates
Q.SL.library = sl_libs, # libraries for initial estimate
g.SL.library = sl_libs) # libraries for prob to be in treatment
# process data
# ----------
data_obs_ltmle <-
data_obs %>%
# need to specify W1, W2, etc
rename(W1 = tax, W2 = age, W3 = crim) %>%
select(W1, W2, W3, A, Y)
# implement ltmle
# ----------
result <- ltmle(data_obs_ltmle, # dataset
Anodes = "A",   # vector that shows treatment
Ynodes = "Y",   # vector that shows outcome
abar = 1)
# view
result
# process data
# ----------
data_obs_ltmle <-
data_obs %>%
# need to specify W1, W2, etc
rename(W1 = tax, W2 = age, W3 = crim) %>%
select(W1, W2, W3, A, Y)
# implement ltmle
# ----------
result <- ltmle(data_obs_ltmle, # dataset
Anodes = "A",   # vector that shows treatment
Ynodes = "Y",   # vector that shows outcome
abar = 1)
# view
result
# create function
# ----------
rexpit <- function(x) rbinom(n=length(x), size=1, prob=plogis(x))
# simulate data
# ----------
n <- 1000
W1 <- rnorm(n)
W2 <- rbinom(n, size=1, prob=0.3)
W3 <- rnorm(n)
A <- rexpit(-1 + 2 * W1 + W3)
Y <- rexpit(-0.5 + 2 * W1^2 + 0.5 * W2 - 0.5 * A + 0.2 * W3 * A - 1.1 * W3)
data <- data.frame(W1, W2, W3, A, Y)
# implement tmle
# ----------
result <- ltmle(data,
Anodes="A",
Lnodes=NULL,      # no Lnodes
Ynodes="Y",
abar=1,
SL.library=sl_libs)
# view
result
# simulate data
# ----------
n <- 1000
W <- rnorm(n)
A1 <- rexpit(W)
L <- 0.3 * W + 0.2 * A1 + rnorm(n)
A2 <- rexpit(W + A1 + L)
Y <- rexpit(W - 0.6 * A1 + L - 0.8 * A2)
data <- data.frame(W, A1, L, A2, Y)
# implement ltmle
# ----------
ltmle(data,
Anodes=c("A1", "A2"),  # two treatment variables
Lnodes="L",            # L indicator needs to be named "L"
Ynodes="Y",            # outcome
abar=c(1, 1),          # treatment indicator in Anodes vector
SL.library = sl_libs)
# Add to this package list for additional SL algorithms
pacman::p_load(
tidyverse,
ggthemes,
ltmle,
tmle,
SuperLearner,
tidymodels,
caret,
dagitty,
ggdag,
here)
heart_disease <- read_csv(here('heart_disease_tmle.csv'))
here()
#Keeping original code in case you need to run it, Kasey
#heart_disease <- read_csv(here('heart_disease_tmle.csv'))
heart_disease <- read_csv(here("Project 8", "heart_disease_tmle.csv"))
new_df <- heart_disease %>% select(-ends_with("_2"))
subset <- heart_disease %>% select(-ends_with("_2"))
heart_disease_SL <- heart_disease %>% select(-ends_with("_2"))
# Add to this package list for additional SL algorithms
pacman::p_load(
tidyverse,
ggthemes,
ltmle,
tmle,
SuperLearner,
tidymodels,
caret,
dagitty,
ggdag,
here)
here()
#Keeping original code in case you need to run it, Kasey
#heart_disease <- read_csv(here('heart_disease_tmle.csv'))
heart_disease <- read_csv(here("Project 8", "heart_disease_tmle.csv"))
# Subsetting to just necessary variables
heart_disease_SL <- heart_disease %>% select(-ends_with("_2"))
# Fit SuperLearner Model
## sl lib
## Train/Test split
## Train SuperLearner
## Risk and Coefficient of each model
## Discrete winner and superlearner ensemble performance
## Confusion Matrix
glimpse(heart_disease_SL)
## sl lib
listWrappers()
# Subsetting to just necessary variables
heart_disease_SL <- heart_disease %>% select(-ends_with("_2"))
#glimpse(heart_disease_SL)
# Fit SuperLearner Model
## sl lib
listWrappers()
# set seed
set.seed(987)
# multiple models
# ----------
sl = SuperLearner(Y = y_train,
X = x_train,
family = binomial(),
SL.library = c('SL.mean',    # Baseline
'SL.glm',     # Baseline for binary outcomes
'SL.glmnet',  # Lasso (and ridge?)
'SL.ranger',  # Random forest
'SL.xgboost'))
## Train/Test split
train <-
# Declare the training set with rsample::training()
training(boston_split)
# Subsetting to just necessary variables
heart_disease_SL <- heart_disease %>% select(-ends_with("_2"))
## sl lib
listWrappers()
# set seed
set.seed(987)
# initial split
# ----------
heart_split_SL <-
initial_split(heart_disease_SL, prop = 3/4) # create initial split (tidymodels)
train <-
# Declare the training set with rsample::training()
training(heart_split_SL)
# y_train
y_train <-
train %>%
# is medv where medv > 22 is a 1, 0 otherwise
mutate(medv = ifelse(medv > 22,
1,
0)) %>%
# pull and save as vector
pull(medv)
View(heart_disease_SL)
View(heart_disease_SL)
# Subsetting to just necessary variables
heart_disease_SL <- heart_disease %>% select(-ends_with("_2"))
#glimpse(heart_disease_SL)
# Fit SuperLearner Model
## sl lib
listWrappers()
# set seed
set.seed(987)
## Train/Test split
# initial split
# ----------
heart_split_SL <-
initial_split(heart_disease_SL, prop = 3/4) # create initial split (tidymodels)
train <-
# Declare the training set with rsample::training()
training(heart_split_SL)
# y_train
y_train <-
train %>%
# is medv where medv > 22 is a 1, 0 otherwise
mutate(mortality = ifelse(mortality > 22,
1,
0)) %>%
# pull and save as vector
pull(mortality)
# x_train
x_train <-
train %>%
# drop the target variable
select(-mortality)
# Testing
# ----------
test <-
# Declare the training set with rsample::training()
testing(heart_split_SL)
# y test
y_test <-
test %>%
mutate(mortality = ifelse(medv > 22,
1,
0)) %>%
pull(mortality)
## sl lib
#listWrappers()
# set seed
set.seed(987)
# initial split
# ----------
heart_split_SL <-
initial_split(heart_disease_SL, prop = 3/4) # create initial split (tidymodels)
train <-
# Declare the training set with rsample::training()
training(heart_split_SL)
# y_train
y_train <-
train %>%
# is medv where medv > 22 is a 1, 0 otherwise
mutate(mortality = ifelse(mortality > 22,
1,
0)) %>%
# pull and save as vector
pull(mortality)
# x_train
x_train <-
train %>%
# drop the target variable
select(-mortality)
# Testing
# ----------
test <-
# Declare the training set with rsample::training()
testing(heart_split_SL)
# y test
y_test <-
test %>%
mutate(mortality = ifelse(medv > 22,
1,
0)) %>%
pull(mortality)
# Subsetting to just necessary variables
heart_disease_SL <- heart_disease %>% select(-ends_with("_2"))
#glimpse(heart_disease_SL)
# Fit SuperLearner Model
## sl lib
#listWrappers()
# set seed
set.seed(987)
## Train/Test split
# initial split
# ----------
heart_split_SL <-
initial_split(heart_disease_SL, prop = 3/4) # create initial split (tidymodels)
train <-
# Declare the training set with rsample::training()
training(heart_split_SL)
# y_train
y_train <-
train %>%
pull(mortality)
# x_train
x_train <-
train %>%
select(-mortality)
# Testing
# ----------
test <-
testing(heart_split_SL)
# y test
y_test <-
test %>%
pull(mortality)
# x test
x_test <-
test %>%
select(-mortality)
# multiple models
# ----------
sl = SuperLearner(Y = y_train,
X = x_train,
family = binomial(),
SL.library = c('SL.mean',    # Baseline
'SL.glm',     # Supposedly a good baseline for binary outcomes
'SL.glmnet',  # Lasso (and ridge?)
'SL.ranger',  # Random forest
'SL.xgboost')) #Because I remember this from earlier in the semester
## Train SuperLearner
## Risk and Coefficient of each model
## Discrete winner and superlearner ensemble performance
## Confusion Matrix
## sl lib
listWrappers()
# multiple models
# ----------
sl = SuperLearner(Y = y_train,
X = x_train,
family = binomial(),
SL.library = c('SL.mean',    # Baseline
'SL.glm',     # Supposedly a good baseline for binary outcomes
'SL.glmnet',  # Lasso (and ridge?)
'SL.ranger',  # Random forest
'SL.xgboost')) #Because I remember this from earlier in the semester
